{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9147df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan direktori kerja saat ini sebagai basis: /home/xerces/project/stemming-project\n",
      "Menginisialisasi stemmer Sastrawi...\n",
      "Stemmer siap.\n",
      "Folder output perbandingan sudah ada: /home/xerces/project/stemming-project/output/sastrawi_comparison_stats\n",
      "Folder output cleaned-only sudah ada: /home/xerces/project/stemming-project/output/cleaned_stats\n",
      "\n",
      "Memulai proses cleaning dan stemming dari folder: /home/xerces/project/stemming-project/input/indo\n",
      "  -> Memproses file: Kel3_Peran Bimbingan dan Konseling Dalam Pendidikan Karakter    .txt...\n",
      "  -> Memproses file: Kel6_Benarkah anak-anak butuh mata pelajaran koding dan AI di sekolah.txt...\n",
      "     WARN: Jumlah kata cleaned (852) != stemmed (857) untuk file Kel6_Benarkah anak-anak butuh mata pelajaran koding dan AI di sekolah.txt. Proxy Stem Collision tidak dihitung.\n",
      "  -> Memproses file: Dampak Tarif Resiprokal Trump terhadap Industri di Indonesia_1.txt...\n",
      "  -> Memproses file: Global South dan Ilusi Netralitas_10.txt...\n",
      "     WARN: Jumlah kata cleaned (1004) != stemmed (1006) untuk file Global South dan Ilusi Netralitas_10.txt. Proxy Stem Collision tidak dihitung.\n",
      "  -> Memproses file: Eksistensi Media Massa Nasional_5.txt...\n",
      "  -> Memproses file: Peran Media Massa dalam Membentuk Opini Publik_5.txt...\n",
      "  -> Memproses file: Kelompok 8_Ini 5 Bahaya Makanan Junk Food yang Perlu Diwaspadai.txt...\n",
      "     WARN: Jumlah kata cleaned (698) != stemmed (697) untuk file Kelompok 8_Ini 5 Bahaya Makanan Junk Food yang Perlu Diwaspadai.txt. Proxy Stem Collision tidak dihitung.\n",
      "  -> Memproses file: Kelompok7_Okupasi Senyap Ruang Angkasa.txt...\n",
      "     WARN: Jumlah kata cleaned (1052) != stemmed (1057) untuk file Kelompok7_Okupasi Senyap Ruang Angkasa.txt. Proxy Stem Collision tidak dihitung.\n",
      "  -> Memproses file: Kel6_Bagaimana mengubah  eco-anxiety kita menjadi aksi untuk Bumi.txt...\n",
      "  -> Memproses file: Kelompok 8_Ketika Jas Putih Menjadi Tameng Menggugat Sistem Pendidikan Dokter Spesialis di Indonesia (1).txt...\n",
      "  -> Memproses file: Kelompok 8_Rupiah di Tengah Perekonomian yang Tertekan.txt...\n",
      "     WARN: Jumlah kata cleaned (791) != stemmed (796) untuk file Kelompok 8_Rupiah di Tengah Perekonomian yang Tertekan.txt. Proxy Stem Collision tidak dihitung.\n",
      "  -> Memproses file: Kel3_Makna Kebersamaan dalam Tradisi Mengibung dan Kembul Bejana.txt...\n",
      "  -> Memproses file: Dugaan Ekspoitasi Mantan Pemain Sirkus, Komisi III DPR Gelar Audiensi Hari ini_2.txt...\n",
      "  -> Memproses file: Kel3_Senarai Catatan di Hari Buku Sedunia dan Tantangan Literasi Indonesia.txt...\n",
      "  -> Memproses file: Kelompok9_Rupiah di Tengah Perekonomian yang Tertekan.txt...\n",
      "     WARN: Jumlah kata cleaned (784) != stemmed (789) untuk file Kelompok9_Rupiah di Tengah Perekonomian yang Tertekan.txt. Proxy Stem Collision tidak dihitung.\n",
      "  -> Memproses file: Gonjang-ganjing Isu Evakuasi Warga Gaza ke Indonesia_2.txt...\n",
      "  -> Memproses file: Korupsi yang Menjegal Daerah_2.txt...\n",
      "  -> Memproses file: Untuk Bantu Pahami Matematika, Anak Perlu Metode Belajar Interaktif_5.txt...\n",
      "  -> Memproses file: Kel6_Kecanduan media sosial bikin anak muda rentan kena gangguan makan.txt...\n",
      "     WARN: Jumlah kata cleaned (702) != stemmed (703) untuk file Kel6_Kecanduan media sosial bikin anak muda rentan kena gangguan makan.txt. Proxy Stem Collision tidak dihitung.\n",
      "  -> Memproses file: Kelompok7_Teknologi Dirgantara Untuk Operasi Bantuan Kemanusiaan.txt...\n",
      "  -> Memproses file: Kelompok9_China Ancam Negara-negara yang Negosiasi Perang Tarif Trump.txt...\n",
      "  -> Memproses file: 4 Calon Kuat Penerus Paus Fransiskus, Ada dari Tetangga RI_10.txt...\n",
      "  -> Memproses file: Melanjutkan Cita-cita Politik Kartini_4.txt...\n",
      "  -> Memproses file: Kelompok7_Pendidikan Dasar dan Menengah yang bermutu untuk seluruh rakyat.txt...\n",
      "     WARN: Jumlah kata cleaned (1046) != stemmed (1047) untuk file Kelompok7_Pendidikan Dasar dan Menengah yang bermutu untuk seluruh rakyat.txt. Proxy Stem Collision tidak dihitung.\n",
      "  -> Memproses file: Merawat Kartini di Kebun Nirkertas_9.txt...\n",
      "     WARN: Jumlah kata cleaned (754) != stemmed (760) untuk file Merawat Kartini di Kebun Nirkertas_9.txt. Proxy Stem Collision tidak dihitung.\n",
      "  -> Memproses file: Ragam Keluhan Mitra MBG Menalangi Biaya hingga Jual Barang Pribadi_Kel 1.txt...\n",
      "  -> Memproses file: AS Soroti QRIS & GPN dalam Negosiasi Dagang, Ini Alasannya_1.txt...\n",
      "  -> Memproses file: TKDN dan Negosiasi Dagang RI-AS_10.txt...\n",
      "  -> Memproses file: Robot Ikut Lomba Lari_4.txt...\n",
      "  -> Memproses file: Dilema Persalinan Caesar di Era JKN Antara Hak Ibu dan Efisiensi Anggaran_4.txt...\n",
      "     WARN: Jumlah kata cleaned (806) != stemmed (808) untuk file Dilema Persalinan Caesar di Era JKN Antara Hak Ibu dan Efisiensi Anggaran_4.txt. Proxy Stem Collision tidak dihitung.\n",
      "\n",
      "Proses cleaning dan stemming selesai.\n",
      "Jumlah file .txt yang diproses: 30\n",
      "Hasil perbandingan disimpan di: /home/xerces/project/stemming-project/output/sastrawi_comparison_stats\n",
      "Hasil cleaning saja disimpan di: /home/xerces/project/stemming-project/output/cleaned_stats\n",
      "Total waktu eksekusi: 73.06 detik\n",
      "\n",
      "--- Evaluasi Statistik TOTAL (Semua File) ---\n",
      "Total kata (tokens) original (semua file): 22977\n",
      "Jumlah kata unik (types) original (semua file): 6529\n",
      "------------------------------\n",
      "Total kata (tokens) setelah cleaning (semua file): 22565\n",
      "Jumlah kata unik (types) setelah cleaning (semua file): 4503\n",
      "------------------------------\n",
      "Total kata (tokens) setelah stemming (semua file): 22596\n",
      "Jumlah kata unik (types) setelah stemming (semua file): 3115\n",
      "------------------------------\n",
      "Total Proxy Stem Collision (semua file): 626\n",
      "------------------------------\n",
      "Persentase reduksi kosakata unik (Original -> Stemmed): 52.29%\n",
      "Persentase reduksi kosakata unik (Cleaned -> Stemmed): 30.82%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import time\n",
    "from collections import Counter, defaultdict # Import defaultdict\n",
    "\n",
    "# --- Konfigurasi ---\n",
    "NAMA_FOLDER_INPUT = 'input/indo'\n",
    "NAMA_FOLDER_OUTPUT_COMPARISON_RELATIF = 'output/sastrawi_comparison_stats' # Nama baru\n",
    "# Hapus atau komentari baris ini jika tidak butuh folder cleaned terpisah\n",
    "NAMA_FOLDER_OUTPUT_CLEANED_RELATIF = 'output/cleaned_stats'\n",
    "# --- Akhir Konfigurasi ---\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "print(f\"Menggunakan direktori kerja saat ini sebagai basis: {base_dir}\")\n",
    "input_folder_path = os.path.join(base_dir, NAMA_FOLDER_INPUT)\n",
    "output_folder_comparison_path = os.path.join(base_dir, NAMA_FOLDER_OUTPUT_COMPARISON_RELATIF)\n",
    "\n",
    "output_folder_cleaned_path = None # Default ke None\n",
    "if 'NAMA_FOLDER_OUTPUT_CLEANED_RELATIF' in locals() and NAMA_FOLDER_OUTPUT_CLEANED_RELATIF:\n",
    "    output_folder_cleaned_path = os.path.join(base_dir, NAMA_FOLDER_OUTPUT_CLEANED_RELATIF)\n",
    "\n",
    "# --- Fungsi Cleaning (sama seperti sebelumnya) ---\n",
    "def bersihkan_teks_preserve_lines(teks):\n",
    "    lines = teks.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    tanda_baca_escaped = re.escape(string.punctuation)\n",
    "    for line in lines:\n",
    "        line = line.lower()\n",
    "        line = re.sub(r\"\\d+\", \"\", line)\n",
    "        line = re.sub(r'[' + tanda_baca_escaped + ']', '', line)\n",
    "        line = re.sub(r'[ \\t]+', ' ', line)\n",
    "        line = line.strip()\n",
    "        cleaned_lines.append(line)\n",
    "    return '\\n'.join(cleaned_lines)\n",
    "# --- Akhir Fungsi Cleaning ---\n",
    "\n",
    "# 1. Inisialisasi Stemmer Sastrawi\n",
    "print(\"Menginisialisasi stemmer Sastrawi...\")\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "print(\"Stemmer siap.\")\n",
    "\n",
    "# 2. Buat folder output\n",
    "if not os.path.exists(output_folder_comparison_path):\n",
    "    os.makedirs(output_folder_comparison_path)\n",
    "    print(f\"Folder output perbandingan dibuat: {output_folder_comparison_path}\")\n",
    "else:\n",
    "    print(f\"Folder output perbandingan sudah ada: {output_folder_comparison_path}\")\n",
    "\n",
    "if output_folder_cleaned_path:\n",
    "    if not os.path.exists(output_folder_cleaned_path):\n",
    "        os.makedirs(output_folder_cleaned_path)\n",
    "        print(f\"Folder output cleaned-only dibuat: {output_folder_cleaned_path}\")\n",
    "    else:\n",
    "        print(f\"Folder output cleaned-only sudah ada: {output_folder_cleaned_path}\")\n",
    "\n",
    "# Variabel Evaluasi TOTAL (untuk console)\n",
    "# ... (variabel total tetap sama) ...\n",
    "unique_words_original_total = set()\n",
    "unique_words_cleaned_total = set()\n",
    "unique_words_stemmed_total = set()\n",
    "total_words_original_all_files = 0\n",
    "total_words_cleaned_all_files = 0\n",
    "total_words_stemmed_all_files = 0\n",
    "total_stem_collisions_all_files = 0 # Tambahan untuk total proxy OI\n",
    "\n",
    "# 3. Proses setiap file\n",
    "print(f\"\\nMemulai proses cleaning dan stemming dari folder: {input_folder_path}\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    if not os.path.isdir(input_folder_path):\n",
    "        raise FileNotFoundError(f\"Folder input '{input_folder_path}' tidak ditemukan.\")\n",
    "\n",
    "    list_file = os.listdir(input_folder_path)\n",
    "    processed_files = 0\n",
    "    skipped_files = 0\n",
    "\n",
    "    if not list_file:\n",
    "        print(\"Folder input kosong.\")\n",
    "\n",
    "    for filename in list_file:\n",
    "        input_file_path = os.path.join(input_folder_path, filename)\n",
    "\n",
    "        if os.path.isfile(input_file_path) and filename.lower().endswith('.txt'):\n",
    "            output_file_comparison_path = os.path.join(output_folder_comparison_path, filename)\n",
    "            current_output_cleaned_path = None\n",
    "            if output_folder_cleaned_path:\n",
    "                current_output_cleaned_path = os.path.join(output_folder_cleaned_path, filename)\n",
    "\n",
    "            print(f\"  -> Memproses file: {filename}...\")\n",
    "\n",
    "            try:\n",
    "                with open(input_file_path, 'r', encoding='utf-8', errors='ignore') as f_in:\n",
    "                    original_text = f_in.read()\n",
    "\n",
    "                # Statistik Original\n",
    "                original_words_list = original_text.split()\n",
    "                count_original_words = len(original_words_list)\n",
    "                # Hanya hitung unik kata non-kosong dari original\n",
    "                set_unique_original = set(w for w in original_words_list if w)\n",
    "                count_unique_original = len(set_unique_original)\n",
    "\n",
    "                # Cleaning\n",
    "                cleaned_text = bersihkan_teks_preserve_lines(original_text)\n",
    "\n",
    "                # Simpan Cleaned (Opsional)\n",
    "                if current_output_cleaned_path:\n",
    "                    with open(current_output_cleaned_path, 'w', encoding='utf-8') as f_clean:\n",
    "                        f_clean.write(cleaned_text)\n",
    "\n",
    "                # Stemming\n",
    "                stemmed_text = stemmer.stem(cleaned_text)\n",
    "\n",
    "                # Statistik Cleaned & Stemmed\n",
    "                cleaned_words_list = cleaned_text.split()\n",
    "                stemmed_words_list = stemmed_text.split()\n",
    "\n",
    "                # Hanya hitung kata/unik non-kosong\n",
    "                valid_cleaned_words = [w for w in cleaned_words_list if w]\n",
    "                valid_stemmed_words = [w for w in stemmed_words_list if w]\n",
    "\n",
    "                count_cleaned_words = len(valid_cleaned_words)\n",
    "                set_unique_cleaned = set(valid_cleaned_words)\n",
    "                count_unique_cleaned = len(set_unique_cleaned)\n",
    "\n",
    "                count_stemmed_words = len(valid_stemmed_words)\n",
    "                set_unique_stemmed = set(valid_stemmed_words)\n",
    "                count_unique_stemmed = len(set_unique_stemmed)\n",
    "\n",
    "                # *** Hitung Proxy \"Stem Collision\" (Indikator Kasar OI) ***\n",
    "                # Buat mapping: stem -> set(kata_unik_cleaned_yg_menghasilkan_stem_ini)\n",
    "                stem_to_cleaned_map = defaultdict(set)\n",
    "                # Asumsi kasar: urutan kata cukup terjaga antara cleaned & stemmed list\n",
    "                # (Ini mungkin tidak 100% akurat jika stemmer menghapus/menggabungkan kata internal)\n",
    "                # Kita pakai list yang sudah divalidasi (non-kosong)\n",
    "                # Perlu panjang list sama agar zip aman, jika tidak sama, lewati proxy ini\n",
    "                num_stem_collisions = 0 # Default 0\n",
    "                if len(valid_cleaned_words) == len(valid_stemmed_words):\n",
    "                    for cleaned_word, stemmed_word in zip(valid_cleaned_words, valid_stemmed_words):\n",
    "                        stem_to_cleaned_map[stemmed_word].add(cleaned_word)\n",
    "\n",
    "                    # Hitung berapa banyak stem yang berasal dari > 1 kata unik cleaned\n",
    "                    for stem, source_words in stem_to_cleaned_map.items():\n",
    "                        if len(source_words) > 1:\n",
    "                            num_stem_collisions += 1\n",
    "                    total_stem_collisions_all_files += num_stem_collisions # Update total\n",
    "                else:\n",
    "                    print(f\"     WARN: Jumlah kata cleaned ({len(valid_cleaned_words)}) \"\n",
    "                          f\"!= stemmed ({len(valid_stemmed_words)}) untuk file {filename}. \"\n",
    "                          f\"Proxy Stem Collision tidak dihitung.\")\n",
    "                # ***********************************************************\n",
    "\n",
    "\n",
    "                # Buat Header File Output (dengan proxy)\n",
    "                file_header = f\"\"\"=============================================\n",
    "FILE: {filename} - STATISTIK\n",
    "=============================================\n",
    "Jumlah Kata (Original): {count_original_words}\n",
    "Jumlah Kata Unik (Original): {count_unique_original}\n",
    "---------------------------------------------\n",
    "Jumlah Kata (Setelah Cleaning): {count_cleaned_words}\n",
    "Jumlah Kata Unik (Setelah Cleaning): {count_unique_cleaned}\n",
    "---------------------------------------------\n",
    "Jumlah Kata (Setelah Stemming): {count_stemmed_words}\n",
    "Jumlah Kata Unik (Setelah Stemming): {count_unique_stemmed}\n",
    "---------------------------------------------\n",
    "Proxy Over-Stemming (Jumlah Stem Collision*): {num_stem_collisions if len(valid_cleaned_words) == len(valid_stemmed_words) else 'N/A'}\n",
    "=============================================\n",
    "* Stem Collision: Jumlah stem yang dihasilkan dari >1 kata unik berbeda setelah cleaning.\n",
    "  Angka tinggi MUNGKIN indikasi over-stemming (perlu cek manual). Dihitung jika jumlah\n",
    "  kata cleaned == stemmed.\n",
    "\n",
    "\"\"\"\n",
    "                separator_cleaned = \"\\n--- TEKS SETELAH CLEANING (SEBELUM STEMMING) ---\\n\"\n",
    "                separator_stemmed = \"\\n\\n--- TEKS SETELAH STEMMING ---\\n\"\n",
    "\n",
    "                final_comparison_content = (\n",
    "                    file_header +\n",
    "                    separator_cleaned +\n",
    "                    cleaned_text +\n",
    "                    separator_stemmed +\n",
    "                    stemmed_text\n",
    "                )\n",
    "\n",
    "                # Tulis hasil perbandingan\n",
    "                with open(output_file_comparison_path, 'w', encoding='utf-8') as f_comparison:\n",
    "                    f_comparison.write(final_comparison_content)\n",
    "\n",
    "                # Update Statistik TOTAL\n",
    "                total_words_original_all_files += count_original_words\n",
    "                unique_words_original_total.update(set_unique_original)\n",
    "                total_words_cleaned_all_files += count_cleaned_words\n",
    "                unique_words_cleaned_total.update(set_unique_cleaned)\n",
    "                total_words_stemmed_all_files += count_stemmed_words\n",
    "                unique_words_stemmed_total.update(set_unique_stemmed)\n",
    "\n",
    "                processed_files += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"     ERROR saat memproses file {filename}: {e}\")\n",
    "        else:\n",
    "            # Log skip file\n",
    "            # ... (log skip file tetap sama) ...\n",
    "            if os.path.isfile(input_file_path):\n",
    "                 print(f\"  -> Melewati file non-txt: {filename}\")\n",
    "            elif os.path.exists(input_file_path):\n",
    "                 print(f\"  -> Melewati item yang bukan file (misal: folder): {filename}\")\n",
    "            else:\n",
    "                 print(f\"  -> Path tidak valid atau tidak ditemukan: {filename}\")\n",
    "            skipped_files += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    # Print status selesai\n",
    "    # ... (print status, jumlah file, path output, waktu) ...\n",
    "    print(f\"\\nProses cleaning dan stemming selesai.\")\n",
    "    print(f\"Jumlah file .txt yang diproses: {processed_files}\")\n",
    "    if skipped_files > 0:\n",
    "        print(f\"Jumlah item non-txt/subfolder yang dilewati: {skipped_files}\")\n",
    "    print(f\"Hasil perbandingan disimpan di: {output_folder_comparison_path}\")\n",
    "    if output_folder_cleaned_path and os.path.exists(output_folder_cleaned_path):\n",
    "         print(f\"Hasil cleaning saja disimpan di: {output_folder_cleaned_path}\")\n",
    "    print(f\"Total waktu eksekusi: {total_time:.2f} detik\")\n",
    "\n",
    "\n",
    "    # --- Cetak Hasil Evaluasi Statistik TOTAL (Semua File) ---\n",
    "    print(\"\\n--- Evaluasi Statistik TOTAL (Semua File) ---\")\n",
    "    count_unique_original_total = len(unique_words_original_total)\n",
    "    count_unique_cleaned_total = len(unique_words_cleaned_total)\n",
    "    count_unique_stemmed_total = len(unique_words_stemmed_total)\n",
    "\n",
    "    print(f\"Total kata (tokens) original (semua file): {total_words_original_all_files}\")\n",
    "    print(f\"Jumlah kata unik (types) original (semua file): {count_unique_original_total}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Total kata (tokens) setelah cleaning (semua file): {total_words_cleaned_all_files}\")\n",
    "    print(f\"Jumlah kata unik (types) setelah cleaning (semua file): {count_unique_cleaned_total}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Total kata (tokens) setelah stemming (semua file): {total_words_stemmed_all_files}\")\n",
    "    print(f\"Jumlah kata unik (types) setelah stemming (semua file): {count_unique_stemmed_total}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Total Proxy Stem Collision (semua file): {total_stem_collisions_all_files}\") # Baru\n",
    "    # --- Perhitungan Persentase Reduksi (sama seperti sebelumnya) ---\n",
    "    print(\"-\" * 30)\n",
    "    # ... (print persentase reduksi) ...\n",
    "    if count_unique_original_total > 0:\n",
    "        reduction_from_original = ((count_unique_original_total - count_unique_stemmed_total) / count_unique_original_total) * 100\n",
    "        print(f\"Persentase reduksi kosakata unik (Original -> Stemmed): {reduction_from_original:.2f}%\")\n",
    "    else:\n",
    "        print(\"Tidak dapat menghitung reduksi dari Original (kosakata original 0).\")\n",
    "\n",
    "    if count_unique_cleaned_total > 0:\n",
    "        reduction_from_cleaned = ((count_unique_cleaned_total - count_unique_stemmed_total) / count_unique_cleaned_total) * 100\n",
    "        print(f\"Persentase reduksi kosakata unik (Cleaned -> Stemmed): {reduction_from_cleaned:.2f}%\")\n",
    "    else:\n",
    "        print(\"Tidak dapat menghitung reduksi dari Cleaned (kosakata cleaned 0).\")\n",
    "\n",
    "\n",
    "except FileNotFoundError as fnf_error:\n",
    "    print(f\"ERROR: {fnf_error}\")\n",
    "    print(\"Pastikan folder input ada.\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan umum: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
